# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uUmatrOeRWXF0brkEDaq3vYjTYRw0KEX
"""





import os
from glob import glob
from torch.utils.data import Dataset
from torchvision import transforms
from pathlib import Path
from PIL import Image
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import wandb
from tqdm import tqdm

# Define transformations
transform = transforms.Compose([
    transforms.Resize((192, 192)),  # Resize to match output size
    transforms.ToTensor()])



class OutpaintingDataset(Dataset):
    def __init__(self, root_dir, transform=None, input_size=128, output_size=192):
        super().__init__()
        self.root_dir = Path(root_dir)
        self.transform = transform
        self.input_size = input_size
        self.output_size = output_size

        # Validate and store image paths
        self.image_paths, self.invalid_images = self._validate_images()

        if not self.image_paths:
            raise RuntimeError(f"No valid images found in {self.root_dir}")

        if self.invalid_images:
            print(f"Found {len(self.invalid_images)} invalid images:")
            for path, error in self.invalid_images:
                print(f"- {path}: {error}")

    def _validate_images(self):
        """Validate images and store valid paths"""
        image_paths = []
        invalid_images = []

        for img_path in self.root_dir.glob("*.[jp][pn]g"):  # Matches jpg, jpeg, png
            try:
                with Image.open(img_path) as img:
                    img.verify()  # Lightweight validation
                    image_paths.append(str(img_path))  # Store as string for indexing
            except Exception as e:
                invalid_images.append((str(img_path), str(e)))

        return image_paths, invalid_images

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        try:
            img = Image.open(img_path).convert("RGB")

            # Check size constraints
            if img.size[0] < self.input_size or img.size[1] < self.input_size:
                raise ValueError(f"Image too small: {img.size}")

            img = img.resize((self.output_size, self.output_size), Image.Resampling.LANCZOS)

            input_img = self._create_input_image(img)
            target_img = self._create_target_image(img)

            return input_img, target_img

        except Exception as e:
            print(f"Error processing image {img_path}: {e}")
            return self._handle_fallback(idx)

    def _handle_fallback(self, idx):
        """Handles fallback when an image fails to load"""
        fallback_idx = (idx + 1) % len(self.image_paths)
        if fallback_idx != idx:
            return self.__getitem__(fallback_idx)

        print("WARNING: Returning blank image due to all fallbacks failing")
        return torch.zeros(3, self.input_size, self.input_size), torch.zeros(3, self.output_size, self.output_size)

    def _create_input_image(self, img):
        """Create input image by center cropping"""
        left = (self.output_size - self.input_size) // 2
        top = (self.output_size - self.input_size) // 2
        right = left + self.input_size
        bottom = top + self.input_size

        input_img = img.crop((left, top, right, bottom))

        return self.transform(input_img) if self.transform else transforms.ToTensor()(input_img)

    def _create_target_image(self, img):
        """Create target image"""
        return self.transform(img) if self.transform else transforms.ToTensor()(img)





dataset_path = "C:/outpaint/venv/dataset/GAN Dataset/GAN Dataset/train/Images"
dataset = OutpaintingDataset(root_dir=dataset_path, transform=transform)

train_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)

print(f"Dataset initialized with {len(dataset)} images.")

import torch
import torch.nn as nn
import torch.nn.functional as F

class OptimizedGenerator(nn.Module):
    def __init__(self, input_channels=3, output_channels=3, feature_dim=64):
        super().__init__()

        # Encoder (Downsampling)
        self.enc1 = nn.Conv2d(input_channels, feature_dim, kernel_size=4, stride=2, padding=1)  # 128 → 64
        self.enc2 = nn.Conv2d(feature_dim, feature_dim * 2, kernel_size=4, stride=2, padding=1)  # 64 → 32
        self.enc3 = nn.Conv2d(feature_dim * 2, feature_dim * 4, kernel_size=4, stride=2, padding=1)  # 32 → 16
        self.enc4 = nn.Conv2d(feature_dim * 4, feature_dim * 8, kernel_size=4, stride=2, padding=1)  # 16 → 8

        # Bottleneck
        self.bottleneck = nn.Conv2d(feature_dim * 8, feature_dim * 8, kernel_size=4, stride=2, padding=1)  # 8 → 4

        # Decoder (Upsampling)
        self.dec4 = nn.ConvTranspose2d(feature_dim * 8, feature_dim * 4, kernel_size=4, stride=2, padding=1)  # 4 → 8
        self.dec3 = nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, kernel_size=4, stride=2, padding=1)  # 8 → 16
        self.dec2 = nn.ConvTranspose2d(feature_dim * 2, feature_dim, kernel_size=4, stride=2, padding=1)  # 16 → 32
        self.dec1 = nn.ConvTranspose2d(feature_dim, output_channels, kernel_size=4, stride=2, padding=1)  # 32 → 64

        self.final_conv = nn.Conv2d(output_channels, output_channels, kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        # Encoder Path
        e1 = F.leaky_relu(self.enc1(x), 0.2)
        e2 = F.leaky_relu(self.enc2(e1), 0.2)
        e3 = F.leaky_relu(self.enc3(e2), 0.2)
        e4 = F.leaky_relu(self.enc4(e3), 0.2)

        # Bottleneck
        b = F.leaky_relu(self.bottleneck(e4), 0.2)

        # Decoder Path
        d4 = F.relu(self.dec4(b))
        d3 = F.relu(self.dec3(d4))
        d2 = F.relu(self.dec2(d3))
        d1 = torch.tanh(self.dec1(d2))  # Tanh activation for image output

        return self.final_conv(d1)

class OptimizedDiscriminator(nn.Module):
    def __init__(self, input_channels=3, feature_dim=64):
        super().__init__()

        self.model = nn.Sequential(
            nn.Conv2d(input_channels, feature_dim, kernel_size=4, stride=2, padding=1),  # 128 → 64
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim, feature_dim * 2, kernel_size=4, stride=2, padding=1),  # 64 → 32
            nn.BatchNorm2d(feature_dim * 2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim * 2, feature_dim * 4, kernel_size=4, stride=2, padding=1),  # 32 → 16
            nn.BatchNorm2d(feature_dim * 4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim * 4, feature_dim * 8, kernel_size=4, stride=2, padding=1),  # 16 → 8
            nn.BatchNorm2d(feature_dim * 8),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=1)  # 8 → 7 (PatchGAN output)
        )

    def forward(self, x):
        return self.model(x)


class Config:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    lr = 2e-4  # Learning rate
    epochs = 50  # Number of epochs
    batch_size = 8  # Adjust for VRAM
    grad_clip = 1.0  # Gradient clipping
    log_interval = 10  # Log after every 10 batches
    eval_interval = 5  # Evaluate every 5 epochs
    lambda_adv = 1.0  # Weight for adversarial loss
    lambda_l1 = 100.0  # Weight for L1 loss
    use_perceptual_loss = False  # Enable perceptual loss if needed
    lambda_perceptual = 0.1  # Weight for perceptual loss

config = Config()

import torch
import torchvision.utils as vutils


def validate_gan(generator, val_loader, device, epoch, save_images=True):
    """
    Runs validation on the generator and returns evaluation metrics.

    Args:
        generator (nn.Module): The trained generator model.
        val_loader (DataLoader): Validation dataset loader.
        device (torch.device): Device to run validation on.
        epoch (int): Current epoch number.
        save_images (bool): Whether to save sample outputs.

    Returns:
        dict: Dictionary containing validation loss and other metrics.
    """
    generator.eval()  # Set to evaluation mode
    total_loss = 0.0
    criterion = torch.nn.L1Loss()  # Using L1 loss for image comparison
    generated_images = []

    with torch.no_grad():  # No gradient calculation for validation
        for i, (input_img, target_img) in enumerate(val_loader):
            input_img, target_img = input_img.to(device), target_img.to(device)

            output_img = generator(input_img)  # Generate output

            loss = criterion(output_img, target_img)  # Compute loss
            total_loss += loss.item()

            if save_images and i < 5:  # Save only a few images for visualization
                generated_images.append((input_img, output_img, target_img))

    avg_loss = total_loss / len(val_loader)  # Compute average loss

    # Save sample images
    if save_images:
        save_validation_images(generated_images, epoch)

    return {"val_loss": avg_loss}


def save_validation_images(image_triplets, epoch, output_dir="validation_samples"):
    """
    Saves validation results as images to track progress.

    Args:
        image_triplets (list): List of (input, generated, target) image sets.
        epoch (int): Current epoch.
        output_dir (str): Directory to save images.
    """
    import os
    from torchvision.utils import save_image

    os.makedirs(output_dir, exist_ok=True)

    for i, (input_img, gen_img, target_img) in enumerate(image_triplets):
        combined = torch.cat([input_img, gen_img, target_img], dim=2)  # Side-by-side
        save_image(combined, f"{output_dir}/epoch_{epoch}_sample_{i}.png")

    print(f"Validation images saved in {output_dir}/")


def train_gan(config, train_loader, val_loader):
    """
    Training loop for the GAN with mixed precision, gradient scaling,
    learning rate scheduling, and best model tracking.
    """
    # Initialize wandb for tracking
    wandb.init(project="outpainting-gan", config=config)

    # Initialize models
    generator = OptimizedGenerator().to(config.device)
    discriminator = OptimizedDiscriminator().to(config.device)

    # Optimizers
    optimizer_G = torch.optim.AdamW(generator.parameters(), lr=config.lr, betas=(0.5, 0.999), weight_decay=0.01)
    optimizer_D = torch.optim.AdamW(discriminator.parameters(), lr=config.lr, betas=(0.5, 0.999), weight_decay=0.01)

    # Learning rate schedulers with warm restarts
    scheduler_G = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_G, T_0=10, T_mult=2)
    scheduler_D = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_D, T_0=10, T_mult=2)

    # Mixed precision setup
    scaler = torch.cuda.amp.GradScaler()

    # Loss functions
    adversarial_loss = nn.BCEWithLogitsLoss()
    l1_loss = nn.L1Loss()

    # Best model tracking
    best_fid = float('inf')

    for epoch in range(config.epochs):
        generator.train()
        discriminator.train()

        epoch_g_loss = 0
        epoch_d_loss = 0
        num_batches = 0

        with tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs}") as pbar:
            for batch_idx, (input_imgs, target_imgs) in enumerate(pbar):
                batch_size = input_imgs.size(0)
                input_imgs, target_imgs = input_imgs.to(config.device), target_imgs.to(config.device)

                real_label = torch.ones(batch_size, 1, device=config.device)
                fake_label = torch.zeros(batch_size, 1, device=config.device)

                # Train Discriminator
                optimizer_D.zero_grad(set_to_none=True)
                with torch.cuda.amp.autocast():
                    real_output = discriminator(target_imgs)
                    d_loss_real = adversarial_loss(real_output, real_label)

                    with torch.no_grad():
                        fake_imgs = generator(input_imgs)
                    fake_output = discriminator(fake_imgs.detach())
                    d_loss_fake = adversarial_loss(fake_output, fake_label)

                    d_loss = 0.5 * (d_loss_real + d_loss_fake)

                scaler.scale(d_loss).backward()
                scaler.step(optimizer_D)
                scaler.update()

                # Train Generator
                optimizer_G.zero_grad(set_to_none=True)
                with torch.cuda.amp.autocast():
                    fake_imgs = generator(input_imgs)
                    fake_output = discriminator(fake_imgs)
                    g_loss_adv = adversarial_loss(fake_output, real_label)
                    g_loss_l1 = l1_loss(fake_imgs, target_imgs)




                    g_loss = (g_loss_adv * config.lambda_adv +
                              g_loss_l1 * config.lambda_l1
                              )

                scaler.scale(g_loss).backward()
                scaler.step(optimizer_G)
                scaler.update()

                # Log batch metrics
                wandb.log({
                    'batch/g_loss': g_loss.item(),
                    'batch/d_loss': d_loss.item(),
                    'batch/lr_g': optimizer_G.param_groups[0]['lr'],
                    'batch/lr_d': optimizer_D.param_groups[0]['lr']
                })

                # Update metrics
                epoch_g_loss += g_loss.item()
                epoch_d_loss += d_loss.item()
                num_batches += 1
                pbar.set_postfix({"G_loss": g_loss.item(), "D_loss": d_loss.item()})

        # Learning rate update
        scheduler_G.step()
        scheduler_D.step()

        # Validation
        if epoch % config.eval_interval == 0:
            generator.eval()
            val_metrics = validate_gan(generator, val_loader, config.device, epoch)

            # Save best model based on FID score
            if val_metrics["fid"] < best_fid:
                best_fid = val_metrics["fid"]
                torch.save(generator.state_dict(), f"best_generator_epoch_{epoch}.pth")
                torch.save(discriminator.state_dict(), f"best_discriminator_epoch_{epoch}.pth")

    wandb.finish()

